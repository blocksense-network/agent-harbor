# Project Enhancement: Per-Process Mount Namespace Simulation for Cross-Platform Branching

## Overview and Motivation

Linux supports **mount namespaces**, which allow different processes to see distinct filesystem mount configurations (each with its own “root” view of the filesystem)[\[1\]](https://man7.org/linux/man-pages/man7/mount_namespaces.7.html#:~:text=Mount%20namespaces%20provide%20isolation%20of,directory%20hierarchies). This enables scenarios where the same absolute path can refer to different content for different processes. For example, a legacy program always reading C:\\floyd\\floyd.ini could be run by two users such that _Alice’s_ instance sees C:\\floyd mapped to her personal config directory, while _Bob’s_ simultaneous instance sees C:\\floyd mapped to Bob’s directory[\[2\]](https://stackoverflow.com/questions/3556166/windows-equivalent-to-linux-namespaces-per-process-filesystem-mounts#:~:text=Linux%20has%20a%20feature%20called,Floyd)[\[3\]](https://stackoverflow.com/questions/3556166/windows-equivalent-to-linux-namespaces-per-process-filesystem-mounts#:~:text=Now%20you%20may%20be%20thinking%2C,any%20other%20path%20we%20like). Such per-process filesystem views are trivial on Linux via namespaces, but have no direct equivalent on Windows or macOS.

**Goal:** Extend our cross-platform user-space filesystem (as defined in the base specification) to simulate Linux’s per-process mount namespace behavior on macOS and Windows. This enhancement will allow **isolated filesystem branches per process**, so that different processes (and their child processes) can operate on _different versions of the filesystem concurrently_ without interfering with each other’s view. In essence, each process can have its own “branch” of the filesystem, achieving strong isolation similar to containers or chroot jails, but implemented in user space.

**New Objective – Per-Process Branch Isolation:** Augment the filesystem’s snapshot and branching features to support **process-scoped views**. A process may “enter” a branch, after which all filesystem operations from that process (and its children) see a modified view of the filesystem unique to that branch. Other processes continue to see the default or their own branch views, ensuring no cross-contamination of file changes or namespace between processes. This is analogous to mount namespaces on Linux, where processes in different namespaces see distinct directory hierarchies[\[1\]](https://man7.org/linux/man-pages/man7/mount_namespaces.7.html#:~:text=Mount%20namespaces%20provide%20isolation%20of,directory%20hierarchies). Our user-space FS will provide this capability on macOS and Windows, which lack native support.

## High-Level Design

### In-Memory Overlay Layer atop the Native Filesystem

To simulate per-process mounts, the filesystem will act as an **in-memory overlay** on top of the native (underlying) filesystem. We introduce an **upper layer** (in-memory user-space FS) that sits over the entire underlying filesystem (the **lower layer**). This overlay will be mounted at a specific mount point (or drive letter) and will present a unified view of the underlying disk content, intercepting all operations. Key aspects of this design:

- **Overlay of the Entire FS Tree:** The user-space FS is mounted such that its root corresponds to a chosen directory that represents the entire native filesystem. On Linux/macOS, this could be a mount point like /mnt/overlay representing the system’s root (/) as the baseline. On Windows, this can be a new drive letter (e.g., S:) corresponding to the content of an existing drive (C:). Essentially, the overlay initial state mirrors the real filesystem’s content within the mount.

- **Copy-on-Write Behavior:** The overlay employs copy-on-write so that modifications by a process do not directly alter the underlying disk. When a file in the lower (real) filesystem is opened for write or modified through the overlay, the FS will first **copy up** that file into the in-memory layer (or a scratch area) and then apply the write to this copy[\[4\]](https://docs.kernel.org/filesystems/overlayfs.html#:~:text=Objects%20that%20are%20not%20directories,of%20a%20symlink%20does%20not). This ensures the lower layer remains untouched; changes are captured in the upper layer. If a file is never modified in a branch, reads are served directly from the underlying filesystem (or a cached copy) to avoid overhead. This design is analogous to Linux’s OverlayFS: modifications go to the upper layer, while unmodified data is read from the lower layer, keeping the base filesystem intact[\[4\]](https://docs.kernel.org/filesystems/overlayfs.html#:~:text=Objects%20that%20are%20not%20directories,of%20a%20symlink%20does%20not).

- **Upper Layer Storage:** The upper layer resides primarily in memory (for speed) with possible spillover to disk (as per the base project design). All created or modified files in a branch are stored in this layer (with copy-on-write from the lower layer as needed). If memory is exhausted or large files are encountered, data may spill to a temporary storage (e.g., a temp directory), as described in the base spec’s memory management section (using an LRU strategy, etc.). This ensures we can overlay even large filesystems without running out of RAM.

- **Unified Namespace and Pass-through:** To applications, the overlay looks like a normal filesystem containing all the usual directories and files of the system. Operations that only read data and metadata will typically pass through to the underlying filesystem (or return cached results). Operations that modify state (write, create, delete) will be intercepted: the FS will create in-memory copies or records of those changes (ensuring the real filesystem isn’t modified). For example, deleting a file in the overlay might be recorded as a “whiteout” (tombstone) in the upper layer so that the file disappears in that branch’s view, while still existing on disk for other branches[\[5\]](https://docs.kernel.org/filesystems/overlayfs.html#:~:text=In%20order%20to%20support%20rm,directories%20are%20always%20opaque). This approach means the overlay FS must manage a **merge view** of upper and lower layers, resolving what each process sees based on its branch state.

### Branching and Per-Process Views

Building on the snapshot/branching capability of the core design, we introduce two new operations in the FS API: **“Create Branch”** and **“Enter Branch.”**

- **Create Branch:** This operation takes a snapshot of the current state of the filesystem (for the calling process or global state) and creates a new **writable branch** from that point. Internally, this is akin to creating a writable snapshot (similar to the base design’s snapshot feature) – no full copy is made of all data, but the branch starts as a logical copy of the state at that moment[\[6\]]. The new branch has its own identifier or name. Initially, the branch’s content equals the parent state (in terms of how reads will see files), thanks to copy-on-write linking of data blocks. Further modifications in this branch will diverge from the parent. Multiple branches can be created over time (forming a tree of versions), but importantly, our system will allow **multiple branches to be active concurrently** (each isolated to different processes). The core will maintain separate in-memory data structures for each branch to track their specific changes (e.g. an independent overlay diff for each branch). All branches share the same lower-layer baseline (unless branched from an already modified state, in which case they share common ancestors’ data as per snapshot logic).

- **Enter Branch:** This operation causes the calling process (and subsequently any child processes it spawns) to switch its view to a specified branch. In practical terms, once a process “enters” a branch, all filesystem calls it makes will be served against that branch’s state. For example, if branch _B1_ has a version of file config.ini that was modified (different from the main branch), a process that has entered _B1_ will read the modified config.ini, whereas other processes not in _B1_ (perhaps in the main branch or another branch) will still see the original content[\[6\]]. Entering a branch is conceptually similar to the process entering a new mount namespace: the process’s view of the filesystem is now isolated from others. Technically, our FS needs to record an association between the process (or its thread/group/session) and the chosen branch.

**Process-Scoped Views:** Once a process has entered a branch, that branch becomes _its_ filesystem view. **Isolation is enforced by the FS core**: changes made in one branch are not visible in any other branch[\[6\]]. If Process A is in branch B1 and creates or modifies files, Process B (in branch B2 or in the default main branch) will not see those changes at all – those modifications are confined to B1’s upper layer. This satisfies the requirement that each branch’s changes stay isolated (the core will isolate file handles, open file states, and data per branch context[\[6\]]). Even if both processes refer to the same file path, they may get different results depending on their branch (just like the “C:\\floyd” example where each user saw their own config file content[\[3\]](https://stackoverflow.com/questions/3556166/windows-equivalent-to-linux-namespaces-per-process-filesystem-mounts#:~:text=Now%20you%20may%20be%20thinking%2C,any%20other%20path%20we%20like)).

**Default Behavior:** By default (if a process does not actively enter a branch), it sees the **main branch**, which is effectively an overlay that tracks global changes or the base state. The main branch initially mirrors the underlying filesystem entirely. System services or other users not using the special branching API will continue to see the main branch (so our FS can be used as a general overlay FS as well). Only processes that explicitly enter an alternate branch get a divergent view. We may designate the main branch as branch “0” or “master” (for instance), and ensure normal operation uses that unless switched.

**Branch Persistence and Scope:** Branches created can persist as long as needed (until deleted), and multiple processes could even enter the same branch if we allow (for collaborative scenarios), though by default branches will be used to isolate one process/group. The branch continues to exist even if no process is currently using it (much like a snapshot that can be mounted later). This means the system needs to manage branch lifecycles: list existing branches, allow deletion of branches when they are no longer needed, etc., as described in the base specification’s snapshot management (e.g., listing snapshots/branches, deleting them to free resources)[\[7\]\[7\]].

**Merging and Interaction:** By design, there is no automatic merging of changes between branches (merging is out of scope). However, a process could choose to take a snapshot of its branch and potentially use that to replace the main branch or another branch manually (effectively “promoting” certain changes). But unless such an explicit action is taken, each branch’s changes remain isolated. This assures that experiments or ephemeral changes in a branch won’t accidentally pollute other views. If needed, one branch can be discarded (deleted) without affecting others, or a process can switch back to the main branch (or another branch) by calling “enter branch” with a different target – effectively **switching active branch** for that process.

### Internal Mechanisms for Branch Isolation

To implement per-process branch isolation, the core filesystem logic will be enhanced in the following ways:

- **Branch Context Tracking:** The core will maintain a mapping of **process identifiers to branch IDs** (or a similar concept of session context). For each incoming filesystem request, the user-space FS will determine which process (or which branch context) is making the request, and will route the operation to the appropriate branch’s view. For example, in a FUSE-based implementation on Linux/macOS, we can use fuse_get_context() to obtain the PID (and UID) of the calling process for each operation, and then look up that PID’s assigned branch. On Windows/WinFsp, if direct PID is not available in callbacks, we may use alternative strategies (see Platform Integration below) – e.g., running separate mount instances per branch, or tagging threads/IRPs with a branch context if possible. The core’s data structures (file index, directory trees, etc.) will effectively be partitioned or duplicated per branch. Each branch has its own modifications log or overlay. Shared underlying data is referenced commonly until a write occurs in a given branch (at which point that branch gets its own private copy of the affected file or metadata).

- **Isolated Open File Handles:** A challenge with branching is handling open file handles across branch switches or snapshots. The design will ensure that file handles are tied to the branch in which they were opened[\[6\]]. For instance, if a process opens a file while in branch B1, that handle will continue to operate on B1’s version of the file even if another snapshot is taken or the process later switches branch (though in practice, a process should not typically switch branches while holding open handles; if it does, the behavior will be defined – likely the open handle remains on the old branch’s state until closed). Internally, we may give each branch its own file/inode identifiers (e.g., a compound ID that includes branch ID) so that handles from different branches don’t collide and the FS can differentiate “file 123 on branch B1” vs “file 123 on branch B2”[\[6\]].

- **Copy-on-Write and Memory Efficiency:** When a branch is created, it shares all data with its parent snapshot until modifications occur. The core will use the snapshot mechanism (described in the base spec) to ensure that unchanged files are not duplicated in memory. For example, if branch B2 is created from main at a time when main has some modified files in its overlay, B2’s state can point to the same memory/disk copies of those files until either branch diverges on them. We will leverage reference counting or persistent data structures so that branches share common subtrees of the filesystem graph where possible[\[8\]]. New writes trigger copy-on-write: if a file in the lower layer is written in branch B1, we copy it into B1’s upper layer (if not already copied)[\[4\]](https://docs.kernel.org/filesystems/overlayfs.html#:~:text=Objects%20that%20are%20not%20directories,of%20a%20symlink%20does%20not). If that same file is later written in branch B2, B2 will get its own copy (or possibly copy from lower again, independent of B1’s changes). Thus, each branch gets an independent version of the file once it modifies it. This ensures isolation and aligns with the snapshot semantics (snapshots are effectively read-only views; branches are writable snapshots).

- **Whiteouts and Deletions:** If a file or directory is deleted in a branch, the deletion should not affect other branches or the real disk. The FS will record a _whiteout_ marker in the branch’s overlay (a technique similar to overlay filesystems[\[5\]](https://docs.kernel.org/filesystems/overlayfs.html#:~:text=In%20order%20to%20support%20rm,directories%20are%20always%20opaque)) so that the file is hidden in that branch. Other branches without that marker will still see the file (from either their overlay or the underlying disk). Implementing whiteouts may involve creating a special record (e.g., a zero-length file with a specific flag in our metadata) indicating the absence of the file in that branch.

- **Concurrent Branch Access:** The FS core must handle that multiple branches might be active at once, potentially being modified concurrently by different processes on different threads. This requires careful concurrency control. We will extend the synchronization mechanisms (locks, etc.) to account for branch-specific data. For instance, updates in branch B1 should lock B1’s relevant structures but should not block read/write in branch B2 except when they contend on shared underlying resources. Shared lower-layer data that’s being copied up to one branch might need a consistency lock so that another branch copying the same base file doesn’t race; however, since branches operate independently, such cases are limited. We will ensure thread-safe handling of branch operations so that one branch’s heavy I/O or snapshot creation doesn’t stall operations in another branch unnecessarily (except for global resources like the underlying disk access which the OS will handle).

## Functional Requirements

- **Complete Filesystem Semantics (Inherited):** The enhanced system must continue to meet all base filesystem requirements (POSIX semantics on macOS, NTFS semantics on Windows, correct handling of reads/writes/locks, etc., as detailed in the original specification). The addition of branches should not break normal operations for processes using the main branch. All standard file operations (create, open, read, write, rename, delete, etc.) must work within a branch as they would on a normal filesystem. For example, a process in an isolated branch can create and modify files freely, list directories, etc., and the behavior (permissions, file locking, etc.) should conform to the OS expectations just as in the base FS[\[9\]].

- **Branch Creation:** Provide an interface (API or command) for authorized processes to create a new branch. Creating a branch should be an **atomic operation** that instantly creates a point-in-time copy of the current branch state without duplicating data. It returns a branch identifier or handle. The operation must be efficient (constant or log-time, independent of filesystem size) – leveraging the snapshot mechanism (using copy-on-write and shared references) rather than physically cloning all files[\[6\]]. There may be limits on the number of branches (for performance and memory), but the design should allow on the order of at least dozens of active branches concurrently.

- **Branch Identification:** Each branch should have a unique identifier (could be a numeric ID, GUID, or human-friendly name). This ID is used when invoking “enter branch” or managing branches. It’s useful to allow branches to be named (e.g., “experiment_A” or “user123_session”) for easier management, though internally it will map to an ID.

- **Enter Branch (Process Isolation):** Provide an interface for a process to enter a branch. This could be a library call in the process or an ioctl/fuse control command from an external tool. When invoked, the filesystem will record that the calling process ID (and any future children) are bound to the specified branch. **All subsequent filesystem calls from that process must resolve against the branch’s state.** This includes path lookups, so effectively the process sees the branch’s namespace as if it were the only filesystem. The operation should also handle updating the process’s environment as needed (see Platform Integration below – e.g., chroot or drive letter usage). Only one branch can be active per process at a time; calling enter branch again can switch the process to a different branch (with the caveat that open file descriptors remain with the old branch’s content until closed, as discussed). Exiting a branch could be done by entering the main branch or another branch, or simply terminating the process (upon which the OS cleans up, and the FS can drop any per-process mappings once no handles from that process remain).

- **Branch Isolation Guarantees:** Changes in one branch must not be visible in any other branch unless explicitly merged. This means:

- Files created in branch X do not exist in branch Y (unless branch Y was derived from X after creation, in which case it would see it as part of its base snapshot).

- If a file is modified in branch X, branch Y continues to see the original version (from their snapshot baseline)[\[6\]].

- Deleting a file in branch X does not remove it for branch Y. Each branch has independent namespace operations.

- The only exception is the base underlying filesystem: if no branch has overridden a file, they all see the same underlying content. (If the underlying filesystem changes externally, those changes will appear in all branches that have not modified that file, as part of their “lower” content – essentially acting like a shared read-only base. However, external modifications of the underlying while the overlay is active can lead to inconsistencies in snapshots. It’s recommended that the underlying FS be treated as static or quiescent while branches are in use, or that appropriate synchronization is implemented if underlying changes are expected. In many use cases, the overlay might be used on a static base image, or the act of mounting the overlay could freeze the baseline view.)

- **Consistency and Crash Safety:** The branch feature must maintain filesystem consistency. Operations within a branch should be atomic and crash-safe to the same degree as in the base FS. For instance, if the system crashes, the state of each branch should reflect either all or none of each completed operation (using journaling or transactional techniques in the core if needed). The snapshot creation should be crash-safe (either a branch exists or it doesn’t). Also, entering a branch either succeeds (the process is fully switched to the new view) or fails cleanly (the process continues in its old view with no partial changes). Any temporary state during branch switching should be well-managed.

- **Performance:** The addition of branching should have minimal overhead on operations:

- Branch lookup (the step of determining which branch a process is in) should be very fast (e.g., a hash map lookup by PID) and ideally negligible compared to I/O costs.

- Copy-on-write operations do add overhead on first write to a file (performing a file copy to memory or temp storage). This should be optimized – e.g., using efficient file copy techniques or even demand-loading portions of a file on write (though simplest is to copy whole file on first write[\[4\]](https://docs.kernel.org/filesystems/overlayfs.html#:~:text=Objects%20that%20are%20not%20directories,of%20a%20symlink%20does%20not), we must do so prudently to avoid large latency spikes; large files could be copied in chunks or use a reference-counted mechanism where only changed parts are duplicated).

- Reading unmodified data incurs a pass-through cost (reading from underlying disk through our user-space layer). We should ensure our read path is well-optimized (possibly implement read buffering or leverage OS page cache through our FUSE/WinFsp integration so that repeated reads aren’t always forwarded to disk once cached).

- There is memory overhead for tracking multiple branches. Data structures like directory trees might be shared until divergence – the design should maximize sharing to reduce memory usage (e.g., use copy-on-write for metadata as well, not just file data, so that thousands of duplicate directory entries across branches aren’t stored multiple times).

- **Security and Access Control:** The per-process isolation should also respect security boundaries:

- On multi-user systems, one user’s branch should not be accessible to another user’s process, unless explicitly allowed. Our FS should integrate with OS permissions (as in base design)[\[11\]]. If two processes of different users enter branches, the usual file permission model still applies within each branch. The act of entering a branch might be restricted (perhaps only certain privileged users or processes can create/enter branches, depending on the use case) – we may implement an access check for branch operations (for example, requiring root or an admin role on macOS/Windows to create new branches on top of system root, since it effectively allows one to bypass or overlay system files).

- The chroot mechanism on macOS (and Linux) inherently requires root privileges to perform. On Windows, mounting a drive or substituting a drive letter might require admin rights or membership in a certain group (WinFsp typically allows non-admin mounts in certain configurations, but not for truly system-wide scenarios). These constraints mean that using this feature is likely an administrative action or part of a controlled environment. We will document the needed privileges and ensure that the FS does not allow a non-privileged user to hijack another’s branch or view data they shouldn’t.

- **User-Space API/Control Interface:** We will extend the FS’s control interface to allow management of branches. Possible approaches include:

- **Ioctls/FS Controls:** e.g., a special FUSE ioctl or WinFsp FSCTL to trigger branch creation or switching (as noted in the base spec, we could define custom control codes like FSCTL_TAKE_SNAPSHOT or FSCTL_SWITCH_BRANCH that our filesystem recognizes[\[12\]]).

- **Special Files:** an alternative is providing a virtual control file (like /.\_fs_control in the mounted FS) where writing a command (like branch create \<name\> or branch switch \<id\>) triggers the operation in the driver. This needs careful design to parse commands and return results.

- **Library API:** since our core is a library (in Rust with C FFI), we could expose functions create_branch() and enter_branch() that applications can call directly if they are linked against our library. For example, a sophisticated application might use the library to manage branches internally.

Regardless of the method, these operations should be designed to be thread-safe and only allowed in appropriate contexts (for instance, you cannot “enter branch” for a different PID unless you have privileges; typically a process can only switch its own branch, whereas an admin tool might orchestrate another process’s environment before launching it).

- **Lifecycle Management:** The system should handle branch lifecycles:

- List existing branches (with metadata such as their base snapshot, creation time, etc.).

- Delete branches that are no longer needed, reclaiming memory and disk space. Deletion of a branch that is currently in use by a process should be disallowed or result in those processes being forced out (likely disallowed to keep things simple – the admin must ensure no process is using a branch before deleting it).

- Possibly persist branch definitions across unmount/mount (though primarily the FS is ephemeral; if the underlying FS hasn’t changed, one could imagine re-playing a saved snapshot log to recover branches on re-mount. This is an advanced feature and may be considered out-of-scope for now, unless needed for long-running usage).

- Handle process termination: when a process exits, the FS can clean up any process-specific state (e.g., remove its PID from branch mapping). The branch itself remains (other processes or a future process could use it), but resources like file handles held by that process will be released. If no processes are using a branch, it simply remains idle in the background.

## Platform Integration Considerations

Implementing per-process isolated mounts requires different strategies on each platform, due to OS differences in namespace and mounting capabilities. Below we outline how the feature will be realized on Linux, macOS, and Windows:

### Linux (for completeness)

Linux natively supports mount namespaces and could achieve per-process views by using unshare(CLONE_NEWNS) and mount the user-space FS separately in each namespace. However, our user-space filesystem already provides an overlay mechanism that can be used similarly. On Linux, the simplest approach for isolation is actually to use the OS feature: \- We can launch or target a process to be isolated by calling unshare \-m (or programmatically using unshare(2)) to give it a private mount namespace[\[13\]](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points#:~:text=Terminal%201%3A)[\[14\]](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points#:~:text=I%20open%20another%20terminal%20now,and%20do%20the%20below%20commands). \- Inside that namespace, we mount our filesystem (via FUSE) at the desired mount point (e.g., / inside the namespace, effectively overlaying the entire root for that process). This yields true per-process (or per-namespace) mount isolation: only that process (and its namespace peers) see that mount[\[15\]](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points#:~:text=The%20files%20,cannot%20see%20or%20browse%20through). Other processes on the system are unaffected and cannot see the mounted overlay or its changes. \- The user-space FS core would still handle branches as described. In this case, one could even choose to mount a specific branch in each namespace (instead of using our internal PID-to-branch mapping). For example, process A in namespace A mounts branch B1 at /, process B in namespace B mounts branch B2 at /. They naturally have fully separate views. This approach uses OS mechanisms for isolation, with our FS providing the branching logic underneath. It is very robust (even root in another namespace cannot see the private mount of another, as shown in Linux examples[\[16\]](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points#:~:text=ls%20,Sep%20%203%2022%3A22)). \- Alternative approach on Linux is to mount the overlay FS once in the initial namespace (e.g., at /mnt/overlay) and use **chroot** for the target processes. A process (with appropriate privilege) could chroot("/mnt/overlay") to confine itself to the overlay view. However, note that unlike true mount namespaces, a chroot jail’s files are still accessible to other processes if they know the path (chroot is not security isolation by itself)[\[17\]](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points#:~:text=1). Still, if only one process is intended to use that environment, chroot is a convenient way to ensure all its path resolutions stay under the overlay FS mount point.

Given Linux’s native capabilities, we have flexibility: for Linux deployments we might leverage unshare to simplify per-process mounting (especially in container contexts). But the core of our enhancement – the branch isolation logic – is implemented in the FS, so it does not strictly require Linux namespaces. Even without using unshare, our FS could manage multiple branch views via internal PID mapping on a single FUSE mount (the core sees the pid and serves different content accordingly). We will primarily focus on the macOS/Windows solutions (since Linux can naturally handle this, and our FS’s internal method is cross-platform anyway).

### macOS Integration

macOS does not have an equivalent to Linux mount namespaces; all mounts are generally visible system-wide. Our approach on macOS will use a combination of our user-space overlay FS and **process chroot** to simulate an isolated namespace:

- The overlay filesystem (implemented via **FSKit** as a File Provider or via macFUSE, depending on approach) will be mounted at a designated location, say /mnt/overlayFS. This mount point represents the root of our overlay. We will likely require elevated privileges to mount a custom filesystem (in macOS, mounting a third-party File Provider or FUSE filesystem requires appropriate rights or user consent). Once mounted, /mnt/overlayFS contains a mirror of the entire macOS filesystem (the lower layer being /).

- **Chroot for Target Process:** To isolate a process, we will perform a chroot("/mnt/overlayFS") for that process (and drop it into its own branch if needed). For example, if we want to launch a sandboxed process, we can fork/exec it under a privileged wrapper that first chroots into /mnt/overlayFS. After this, the process’s notion of the root directory / is actually our overlay FS. The process will operate as if it’s on a normal system, but every filesystem operation is now intercepted by our user-space FS. We pass the branch ID to the FS by invoking an “enter branch” call (which could be done just before dropping privileges in the chrooted process, or via an ioctl after chroot). Once that’s done, the process is in branch X and continues execution.

- **Ensuring Branch Consistency:** If multiple processes on macOS use this mechanism simultaneously (say two isolated processes), by default they’d all be chrooting into the _same_ mount point. Without additional measures, they’d all initially see the same branch (the main branch) content. To have separate branches concurrently, we have two possible strategies:

- **Single Mount, FS-internal Branch Mapping:** Mount one overlay FS at /mnt/overlayFS. Different processes chroot to it and then call enter branch \<ID\> for their respective branch. The FS core uses the calling process’s identity to serve the correct branch data. This requires that our FS **can distinguish requests per process**. In the FSKit provider model, we may not get a direct PID for each operation callback. However, since each chrooted process has its own file descriptor namespace, we might track branch at the time of chroot/enter by storing a token in the process (perhaps via an environment variable or a small file that the FS can read to identify branch – this is a bit of a hack). More robustly, we could limit to one process per branch mount on macOS to avoid this complexity.

- **Multiple Mounts (One per Branch):** We could mount the overlay FS multiple times at different mount points, each instance fixed to a specific branch. For example, mount branch B1’s view at /mnt/overlayFS_B1 and branch B2 at /mnt/overlayFS_B2. Then chroot Process1 into /mnt/overlayFS_B1 and Process2 into /mnt/overlayFS_B2. This way, even though macOS doesn’t have per-process namespaces, we manually ensure each isolated process only sees its designated mount. The downside is overhead of multiple mounts and the need for multiple FS instances (our core library can handle that, but it multiplies the number of active FUSE/FSKit instances). Still, this approach aligns with the Option B mentioned in the base design (“mounting snapshot branches at separate mountpoints” for concurrent access)[\[18\]]. We will likely implement either approach depending on capability: if FSKit allows multiple mounts easily, we can go with per-branch mountpoints for simplicity; if not, we enhance our core to multiplex branches on one mount.

- **Privileges:** The act of chrooting requires root privileges on macOS. This means the typical usage pattern is that an administrator or a root-owned orchestrator sets up the environment (mount the FS and spawn processes in chroot). This feature can be used in scenarios like building lightweight macOS containers or sandboxing applications with a custom filesystem view. We will provide tooling or scripts to automate this (e.g., a small launcher program that does the mount, branch creation, chroot, and exec of the target app).

- **Mac-specific File Provider Considerations:** If using the new File Provider API (which is how modern macOS user-space filesystems are implemented), our FS might run in a system extension context. We would integrate branch control into that extension. We may need to implement a custom command interface for branch operations (since FSKit might not natively understand “snapshot” or “branch” concepts). Possibly, we’ll expose a control interface via IPC to the extension (e.g., using XPC between a controlling app and the file provider extension) to signal branch creation or switching. The extension, running with appropriate privileges, can then perform the chroot for a target process if needed or at least set up separate volume for each branch. This area will require some prototyping on macOS since the File Provider mechanism is quite sandboxed itself. It might turn out easier on macOS to use macFUSE for this particular feature (since macFUSE operates more like Linux FUSE and might allow getting the PID context). We will evaluate the best approach during design, but from a requirements perspective, we assume it is feasible to achieve the needed isolation either way.

### Windows Integration

Windows does not support an exact analogue of mount namespaces, but we can leverage **WinFsp** (Windows FUSE-like user-space file system driver) to create alternate drive views, and use Windows-specific mechanisms to confine processes to those views:

- **Per-Drive Overlay:** We will mount our user-space filesystem as one or more **drive letters** or **NTFS folder mount points**. For example, if the real system drive is C:, we might mount the overlay as a new drive S: that represents the same filesystem content as C: (initially). Our FS will intercept operations on S: and apply branching rules. If the system has multiple drives (C, D, etc.) and we need to mirror all, we could create multiple overlay mounts (S:, T:, etc.) or possibly one composite mount that encompasses all (Windows also supports volume mount points, so conceivably we could have one drive that contains mount points for others). Simpler is one-to-one mapping: each underlying drive letter can be given a corresponding overlay drive letter for processes to use. For instance, when a process enters a branch, we might present to it that “its C: drive” is now the overlay mounted at S:. The process would have to use S: instead of C: to see its isolated changes. (We cannot easily hide C: from the process without going into OS-level isolation like user sessions or containers, but we can arrange that the process is launched in an environment where it uses the alternate letters.)

- **Launching Process in Branch:** To get a process to use the overlay, one approach is to launch it in a modified environment where references to the filesystem go to our mount. There are a few techniques:

- The simplest is to adjust what paths the process uses. For example, if we control the working directory or how the process is invoked, we ensure it uses S:\\ for file paths (perhaps by substituting environment variables or command-line arguments). However, for truly transparent operation (like the floyd example, which always refers to C:\\ path internally), we need a stronger isolation.

- Another approach is to leverage **NT Object Namespace** tricks: Windows allows defining **symlinks** or **subst** drives that are local to a logon session. Using DefineDosDevice() with the DDD_PER_SESSION flag, one can create a drive letter mapping that is only visible in the current user session. We could potentially set up a mapping such that in the target process’s session, references to C: actually point to our overlay device. This is somewhat analogous to a per-session namespace. It’s tricky to do for C: directly (Windows might not allow unmapping the real C: easily), but it might be possible in a controlled environment or using a Job object with a silo (though siloed namespace is more for Registry).

- A more practical route: run the target program under a different user account or a **Windows container**. Windows containers (or techniques like RunDLL with “AppContainers”) can have virtualized filesystems. We could mount our overlay as a **virtual disk** and inside a container environment designate that as the system drive. However, using full containers might be beyond our scope (they are heavy and require Windows container feature enabled).

For our requirements, we assume a scenario where we can convince the process to use the new drive letter for its operations. For instance, if we’re sandboxing a tool or running tests, we can configure that tool to use S: as its root drive (maybe via configuration). If the process cannot be configured, a wrapper or shim may be needed (outside the FS’s scope) to intercept file calls (there are user-mode APIs to intercept and remap paths, like a compatibility layer). Such a wrapper could catch calls to C:\\... and rewrite them to S:\\.... Though this is outside the filesystem itself, it’s worth noting as part of the overall solution.

- **WinFsp Implementation:** With WinFsp, we will implement the overlay logic described in the core. WinFsp will allow us to mount a volume with a drive letter (or a NTFS folder path or UNC path). We can run multiple instances of our FS in user mode – WinFsp supports many simultaneous mounts[\[19\]](https://winfsp.dev/doc/Frequently-Asked-Questions/#:~:text=Frequently%20Asked%20Questions%20,processes%20can%20create%20file%20systems). For each branch, we could spin up an instance if needed (similar to the multi-mount approach on macOS). However, running a separate instance for each branch might be heavy if branches are numerous. We likely prefer a single instance of the FS that is aware of multiple branches internally, and possibly expose each branch via a distinct view:

- **Multiple Drive Letters via Same FS Instance:** WinFsp has a concept of network provider which might allow the same process to register multiple distinct mount points. Or we could run a lightweight separate process per branch using the same core library (since our core is FFI, spawning multiple host processes each mounting one branch is feasible).

- For example, when CreateBranch is called, our software could automatically mount a new drive for that branch. Suppose B1 gets mounted at S: and B2 at T:. We then launch process A with access to S: and process B with access to T:. This is explicit but ensures isolation. The core library can share memory between instances or use copy-on-write across instances if we engineer it, or simpler, each instance consults the same underlying base (the real disk) and we devise a way for branches to share data (this might be complex if separate processes; maybe better to have one process multi-mount).

- It might be simpler to implement one FS process that handles one branch at a time per mount. So from a requirements perspective, we assume either design is acceptable as long as branch views are isolated. The user of the system might not care if under the hood it’s one multi-branch FS or many FS instances – but we will aim for efficient resource usage.

- **Drive Letter Management:** We need a scheme for assigning drive letters or mount points for branches. This could be done automatically (first branch \= S:, next \= T:, etc., or using a naming pattern if using UNC, like \\\\BranchFs\\BranchName\\). We must be careful not to clash with existing drives. Possibly allow specifying the mount target when creating a branch or entering a branch. For example, an API could be enter_branch(branch_id, mount_point="S:"). The system would mount the branch there if not already mounted, and then the caller would use that mount. We’ll include in the requirements that the system must handle this mapping transparently or provide the necessary info to the user.

- **Process Isolation on Windows:** Unlike macOS/Linux, simply mounting at a different letter does not restrict other processes from seeing that letter. If two processes share the same user account and one is using S:, the other could also access S: (unless we implement some access control). Windows does allow setting ACLs on volumes and mount points. We could potentially set an ACL on the drive object to only allow a certain user or process to access it. For instance, if we run the branch mount as a specific user (one could create a user account per branch or use an NT security identifier to lock it). However, this might complicate usage. Alternatively, since this is for isolation but not high-security multi-tenant scenarios (likely), we assume other processes simply won’t use the unintended drive. If security is a concern, the proper solution on Windows would involve running the isolated process under a different user or session that only has the branch drive mapped, as mentioned earlier.

- **Example Use Case on Windows:** Suppose we want to test an installer in an isolated environment. We create a branch B_test from the current state, mount it as S:. We then launch the installer program but trick it to install to S:\\Program Files\\... instead of C:\\Program Files (maybe by running the installer in a command prompt where we temporarily rename the drives, or by editing its response file). The installer writes many files; all those writes go to branch B_test’s overlay (leaving C: untouched). After installation, we can compare the branch’s overlay to see what changed, or discard it if we like. Meanwhile, the real system was never affected. Another simultaneous branch could be created for a different test. This is the kind of scenario this feature enables on Windows, albeit requiring careful setup.

### Additional Considerations

- **Cleanup and Unmounting:** When a branch is no longer needed (and no process is using it), we should allow unmounting its view (especially on Windows where each branch might be a separate drive). On macOS, if we used one mount with chroots, there’s just one mount to unmount when the whole system is done. If multiple, unmount per branch as needed. The system should ensure no data loss on unmount – if branches are ephemeral, we just drop them. If we wanted to persist changes from a branch back to disk, that would be a controlled procedure outside normal operation (like a “merge branch” or using the branch’s data to replace the base, which is not automated in this design).

- **Visibility of Mounts:** On macOS, an overlay mount point (like /mnt/overlayFS) is globally visible, but processes outside the chroot will typically not use it unless an admin examines it. On Windows, new drive letters will appear in Explorer by default. This could confuse users if they see drives like S: appear. One way to mitigate this is mounting the file system as a **network drive** or with a special label to indicate its purpose (WinFsp allows mounting as a UNC path only, which might hide it from the general UI). Alternatively, we could mount with the "hidden" attribute or in a way that doesn’t auto-open. This is a usability detail: requirements should note that _the solution should minimize interference with normal user environment_. Perhaps only advanced users or automated systems will use this anyway.

- **Testing:** The per-process branching feature will require extensive testing on each platform:

- Start a process in branch, verify it sees expected file versions, and that another process in a different branch or outside sees different content or the original content.

- Test creating files, deleting, modifying in one branch and ensuring the other branch’s view is unchanged.

- Simulate concurrent writes to the same base file from two branches and ensure each gets its own version without corrupting each other.

- Ensure that leaving a branch (or process exit) and re-entering doesn’t leak old state (e.g., stale file handles).

- On Windows, test that an application actually writes to the overlay drive and not inadvertently to the real drive (this may involve monitoring with ProcMon or similar).

- On macOS, test chroot environments to ensure processes truly cannot escape to the real filesystem (for example, trying to access a path outside the chroot should fail).

In summary, this enhancement adds a powerful **namespacing** capability to our cross-platform filesystem. It leverages the existing snapshot/branch infrastructure[\[6\]], combining it with an overlay technique to present per-process customized views. The end result is akin to lightweight virtual filesystems for each process, all backed by one unified system in memory. This will enable use cases like isolated build environments, testing with different file versions, multi-user sandboxes on the same machine, and more, on platforms that traditionally don’t support such features. It bridges the gap between Linux containers and the Windows/macOS world by using our user-space filesystem as the vehicle for isolation.

---

[\[1\]](https://man7.org/linux/man-pages/man7/mount_namespaces.7.html#:~:text=Mount%20namespaces%20provide%20isolation%20of,directory%20hierarchies) mount_namespaces(7) \- Linux manual page

[https://man7.org/linux/man-pages/man7/mount_namespaces.7.html](https://man7.org/linux/man-pages/man7/mount_namespaces.7.html)

[\[2\]](https://stackoverflow.com/questions/3556166/windows-equivalent-to-linux-namespaces-per-process-filesystem-mounts#:~:text=Linux%20has%20a%20feature%20called,Floyd) [\[3\]](https://stackoverflow.com/questions/3556166/windows-equivalent-to-linux-namespaces-per-process-filesystem-mounts#:~:text=Now%20you%20may%20be%20thinking%2C,any%20other%20path%20we%20like) Windows equivalent to Linux namespaces (per-process filesystem mounts)? \- Stack Overflow

[https://stackoverflow.com/questions/3556166/windows-equivalent-to-linux-namespaces-per-process-filesystem-mounts](https://stackoverflow.com/questions/3556166/windows-equivalent-to-linux-namespaces-per-process-filesystem-mounts)

[\[4\]](https://docs.kernel.org/filesystems/overlayfs.html#:~:text=Objects%20that%20are%20not%20directories,of%20a%20symlink%20does%20not) [\[5\]](https://docs.kernel.org/filesystems/overlayfs.html#:~:text=In%20order%20to%20support%20rm,directories%20are%20always%20opaque) Overlay Filesystem — The Linux Kernel documentation

[https://docs.kernel.org/filesystems/overlayfs.html](https://docs.kernel.org/filesystems/overlayfs.html)

[\[13\]](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points#:~:text=Terminal%201%3A) [\[14\]](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points#:~:text=I%20open%20another%20terminal%20now,and%20do%20the%20below%20commands) [\[15\]](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points#:~:text=The%20files%20,cannot%20see%20or%20browse%20through) [\[16\]](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points#:~:text=ls%20,Sep%20%203%2022%3A22) [\[17\]](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points#:~:text=1) linux \- per process private file system mount points \- Unix & Linux Stack Exchange

[https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points](https://unix.stackexchange.com/questions/153665/per-process-private-file-system-mount-points)

[\[19\]](https://winfsp.dev/doc/Frequently-Asked-Questions/#:~:text=Frequently%20Asked%20Questions%20,processes%20can%20create%20file%20systems) Frequently Asked Questions \- WinFsp

[https://winfsp.dev/doc/Frequently-Asked-Questions/](https://winfsp.dev/doc/Frequently-Asked-Questions/)
